---
title: 분류 알고리즘
description: 고급 통계 모델을 구현하는 데 도움이 되는 주요 매개 변수, 설명 및 예제 코드를 사용하여 다양한 분류 알고리즘을 구성하고 최적화하는 방법에 대해 알아봅니다.
role: Developer
exl-id: 9105ab04-b480-48a0-b8f7-cf0ed5e5399d
source-git-commit: 489063fcd003e20f233a9c9d85d8cb6c22708d88
workflow-type: tm+mt
source-wordcount: '2449'
ht-degree: 4%

---

# 분류 알고리즘 {#classification-algorithms}

이 문서에서는 다양한 분류 알고리즘의 구성, 주요 매개 변수 및 고급 통계 모델의 실용적인 사용에 초점을 맞춰 개요를 제공합니다. 분류 알고리즘을 사용하여 입력 기능에 따라 데이터 포인트에 카테고리를 할당합니다. 각 섹션에는 의사 결정 트리, 랜덤 포레스트 및 나이브 베이즈 분류와 같은 작업에 이러한 알고리즘을 구현하고 최적화하는 데 도움이 되는 매개 변수 설명과 예제 코드가 포함되어 있습니다.

## [!DNL Decision Tree Classifier] {#decision-tree-classifier}

[!DNL Decision Tree Classifier]은(는) 통계, 데이터 마이닝 및 기계 학습에 사용되는 지도 학습 방법입니다. 이 접근법에서는, 결정 트리가 분류 작업들에 대한 예측 모델로서 이용되며, 관측치들의 집합으로부터 결론을 도출한다.

**매개 변수**

아래 표에는 [!DNL Decision Tree Classifier]의 성능을 구성하고 최적화하기 위한 주요 매개 변수가 나와 있습니다.

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|------------------|
| `MAX_BINS` | 최대 빈 수는 연속 피쳐가 어떻게 이산적인 간격으로 분할되는가를 결정합니다. 이 기능은 각 의사 결정 트리 노드에서 기능이 분할되는 방식에 영향을 줍니다. 저장소가 많을수록 세부기간이 높아집니다. | 32 | 모든 카테고리적 기능에 있는 카테고리 수와 같거나 2개 이상이어야 합니다. |
| `CACHE_NODE_IDS` | `false`이면 알고리즘이 트리를 실행자에게 전달하여 인스턴스와 노드를 일치시킵니다. `true`이면 알고리즘이 각 인스턴스에 대한 노드 ID를 캐시하여 더 깊은 트리의 훈련 속도를 높입니다. | `false` | `true`, `false` |
| `CHECKPOINT_INTERVAL` | 캐시된 노드 ID를 체크포인팅하는 빈도를 지정합니다. 예를 들어 `10`은(는) 10번 반복할 때마다 캐시가 검사점을 지정함을 의미합니다. | 10 | (>= 1) |
| `IMPURITY` | 정보 이익 계산에 사용되는 기준(대소문자 구분 안 함). | &quot;gini&quot; | `entropy`, `gini` |
| `MAX_DEPTH` | 트리의 최대 깊이입니다(음수가 아님). 예를 들어, 깊이 `0`은(는) 1개의 리프 노드, 깊이 `1`은(는) 1개의 내부 노드 및 2개의 리프 노드를 의미합니다. | 5 | (>= 0) (범위: [0,30]) |
| `MIN_INFO_GAIN` | 분할이 트리 노드에서 고려되는 데 필요한 최소 정보 이득입니다. | 0.0 | (>= 0.0) |
| `MIN_WEIGHT_FRACTION_PER_NODE` | 분할 후 각 소아가 보유해야 하는 가중치가 적용된 표본 수의 최소 비율입니다. 분할로 인해 하위 항목 중 하나에 있는 총 중량의 일부가 이 값보다 작아지면 무시됩니다. | 0.0 | (>= 0.0, &lt;= 0.5) |
| `MIN_INSTANCES_PER_NODE` | 분할 후 각 하위 항목이 가져야 하는 최소 인스턴스 수입니다. 분할로 인해 이 값보다 적은 인스턴스가 생성되면 분할이 무시됩니다. | 1 | (>= 1) |
| `MAX_MEMORY_IN_MB` | 히스토그램 집계에 할당된 최대 메모리(MB)입니다. 이 값이 너무 작으면 반복당 하나의 노드만 분할되고 해당 집계는 이 크기를 초과할 수 있습니다. | 256 | (>= 0) |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `SEED` | 무작위 시드. | N/A | 모든 64비트 숫자 |
| `WEIGHT_COL` | 열 이름(예: 가중치)입니다. 설정되지 않았거나 비어 있으면 모든 인스턴스 가중치는 `1.0`(으)로 처리됩니다. | 설정되지 않음 | 모든 문자열 |
| `ONE_VS_REST` | 이 알고리즘을 다중 클래스 분류 문제에 사용되는 One-vs-Rest로 래핑하거나 비활성화합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
Create MODEL modelname OPTIONS(
  type = 'decision_tree_classifier'
) AS
  select col1, col2, col3 from training-dataset
```

## [!DNL Factorization Machine Classifier] {#factorization-machine-classifier}

[!DNL Factorization Machine Classifier]은(는) 일반 그라데이션 디센션 및 AdamW 솔버를 지원하는 분류 알고리즘입니다. 인수분해 기계 분류 모델은 로지스틱 손실을 사용하며, 이는 구배 하강을 통해 최적화할 수 있으며, 종종 과적합을 방지하기 위해 L2와 같은 정규화 용어를 포함합니다.

**매개 변수**

아래 표에는 [!DNL Factorization Machine Classifier]의 성능을 구성하고 최적화하는 데 필요한 주요 매개 변수가 나와 있습니다.

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-------------------------------------------------------------------------------------------------------|
| `TOL` | 수렴공차, 최적화의 정확도 제어. | `1E-6` | (>= 0) |
| `FACTOR_SIZE` | 요소의 차원입니다. | 8 | (>= 0) |
| `FIT_INTERCEPT` | 절편 용어를 맞출지 여부를 지정합니다. | `true` | `true`, `false` |
| `FIT_LINEAR` | 선형 항(일방향 항이라고도 함)을 맞추는지 여부를 지정합니다. | `true` | `true`, `false` |
| `INIT_STD` | 계수를 초기화하기 위한 표준 편차입니다. | 0.01 | (>= 0) |
| `MAX_ITER` | 알고리즘을 실행할 최대 반복 횟수입니다. | 10 | (>= 0) |
| `MINI_BATCH_FRACTION` | 교육 중 미니배치로 사용할 데이터의 비율입니다. `(0, 1]` 범위에 있어야 합니다. | 1.0 | 0 &lt; 값 &lt;= 1 |
| `REG_PARAM` | 모델 복잡성을 제어하고 오버피팅을 방지하는 데 도움이 되는 정규화 매개변수입니다. | 0.0 | (>= 0) |
| `SEED` | 알고리즘에서 무작위 프로세스를 제어하기 위한 무작위 시드. | N/A | 모든 64비트 숫자 |
| `SOLVER` | 최적화에 사용되는 해 찾기 알고리즘. 지원되는 옵션은 `gd`(그라데이션 디센트) 및 `adamW`입니다. | &quot;adamW&quot; | `gd`, `adamW` |
| `STEP_SIZE` | 최적화를 위한 초기 단계 크기는 종종 학습률로 해석됩니다. | 1.0 | > 0 |
| `PROBABILITY_COL` | 예측된 클래스 조건부 확률에 대한 열 이름입니다. 참고: 모든 모델이 잘 보정된 확률을 출력하는 것은 아닙니다. 이는 정확한 확률보다는 신뢰도 점수로 처리되어야 합니다. | &quot;probability&quot; | 모든 문자열 |
| `PREDICTION_COL` | 예측된 클래스 레이블의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 모든 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 One-vs-Rest를 사용할지 여부를 지정합니다. | FALSE | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
CREATE MODEL modelname OPTIONS(
  type = 'factorization_machines_classifier'
) AS
  SELECT col1, col2, col3 FROM training-dataset
```

## [!DNL Gradient Boosted Tree Classifier] {#gradient-boosted-tree-classifier}

[!DNL Gradient Boosted Tree Classifier]은(는) 의사 결정 트리의 앙상블을 사용하여 분류 작업의 정확도를 높이고 여러 트리를 결합하여 모델 성능을 향상시킵니다.

**매개 변수**

아래 표에는 [!DNL Gradient Boosted Tree Classifier]의 성능을 구성하고 최적화하는 데 필요한 주요 매개 변수가 나와 있습니다.

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|------------------------------------------------------------------------------------------------------|
| `MAX_BINS` | 최대 빈 수는 연속 피쳐가 어떻게 이산적인 간격으로 분할되는가를 결정합니다. 이 기능은 각 의사 결정 트리 노드에서 기능이 분할되는 방식에 영향을 줍니다. 저장소가 많을수록 세부기간이 높아집니다. | 32 | 은(는) 적어도 2개이어야 하며 모든 카테고리 기능에 있는 카테고리 수보다 크거나 같아야 합니다. |
| `CACHE_NODE_IDS` | `false`이면 알고리즘이 트리를 실행자에게 전달하여 인스턴스와 노드를 일치시킵니다. `true`이면 알고리즘이 각 인스턴스에 대한 노드 ID를 캐시하여 더 깊은 트리의 훈련 속도를 높입니다. | `false` | `true`, `false` |
| `CHECKPOINT_INTERVAL` | 캐시된 노드 ID를 체크포인팅하는 빈도를 지정합니다. 예를 들어 `10`은(는) 10번 반복할 때마다 캐시가 검사점을 지정함을 의미합니다. | 10 | (>= 1) |
| `MAX_DEPTH` | 트리의 최대 깊이입니다(음수가 아님). 예를 들어, 깊이 `0`은(는) 1개의 리프 노드, 깊이 `1`은(는) 1개의 내부 노드 및 2개의 리프 노드를 의미합니다. | 5 | (>= 0) |
| `MIN_INFO_GAIN` | 분할이 트리 노드에서 고려되는 데 필요한 최소 정보 이득입니다. | 0.0 | (>= 0.0) |
| `MIN_WEIGHT_FRACTION_PER_NODE` | 분할 후 각 소아가 보유해야 하는 가중치가 적용된 표본 수의 최소 비율입니다. 분할로 인해 하위 항목 중 하나에 있는 총 중량의 일부가 이 값보다 작아지면 무시됩니다. | 0.0 | (>= 0.0, &lt;= 0.5) |
| `MIN_INSTANCES_PER_NODE` | 분할 후 각 하위 항목이 가져야 하는 최소 인스턴스 수입니다. 분할로 인해 이 값보다 적은 인스턴스가 생성되면 분할이 무시됩니다. | 1 | (>= 1) |
| `MAX_MEMORY_IN_MB` | 히스토그램 집계에 할당된 최대 메모리(MB)입니다. 이 값이 너무 작으면 반복당 하나의 노드만 분할되고 해당 집계는 이 크기를 초과할 수 있습니다. | 256 | (>= 0) |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `VALIDATION_INDICATOR_COL` | 열 이름은 각 행이 교육에 사용되는지 또는 유효성 검사에 사용되는지를 나타냅니다. 값 `false`은(는) 교육을 나타내며 `true`은(는) 유효성 검사를 나타냅니다. 값을 설정하지 않으면 기본값은 `None`입니다. | &quot;없음&quot; | 모든 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 모든 문자열 |
| `LEAF_COL` | 사전 순서에서 생성된 각 트리의 각 인스턴스에 대해 예측된 리프 인덱스인 리프 인덱스에 대한 열 이름입니다. | &quot;&quot; | 모든 문자열 |
| `FEATURE_SUBSET_STRATEGY` | 각 트리 노드에서 분할할 것으로 간주되는 기능의 수입니다. 지원되는 옵션: `auto`(작업에 따라 자동으로 결정됨), `all`(모든 기능 사용), `onethird`(기능 수의 1/3 사용), `sqrt`(기능 수의 제곱근 사용), `log2`(기능 수의 기본-2 로그 사용) 및 `n`(여기서 n은 `(0, 1]` 범위에 있는 기능의 일부이거나 `[1, total number of features]` 범위에 있는 특정 기능 수임). | &quot;auto&quot; | `auto`, `all`, `onethird`, `sqrt`, `log2`, `n` |
| `WEIGHT_COL` | 열 이름(예: 가중치)입니다. 설정되지 않았거나 비어 있으면 모든 인스턴스 가중치는 `1.0`(으)로 처리됩니다. | 설정되지 않음 | 모든 문자열 |
| `LOSS_TYPE` | [!DNL Gradient Boosted Tree] 모델이 최소화하려는 손실 함수입니다. | &quot;logistic&quot; | `logistic`(대/소문자 구분 안 함) |
| `STEP_SIZE` | 각 추정기의 기여도를 축소하는 데 사용되는 `(0, 1]` 범위의 단계 크기(학습 속도라고도 함)입니다. | 0.1 | (>= 0.0, &lt;= 1) |
| `MAX_ITER` | 알고리즘에 대한 최대 반복 횟수입니다. | 20 | (>= 0) |
| `SUBSAMPLING_RATE` | 각 의사 결정 트리를 교육하는 데 사용되는 교육 데이터의 비율입니다. 값은 0 &lt; 값 &lt;= 1 범위에 있어야 합니다. | 1.0 | `(0, 1]` |
| `PROBABILITY_COL` | 예측된 클래스 조건부 확률에 대한 열 이름입니다. 참고: 모든 모델이 잘 보정된 확률을 출력하는 것은 아닙니다. 이는 정확한 확률보다는 신뢰도 점수로 처리되어야 합니다. | &quot;probability&quot; | 모든 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 이 알고리즘을 One-vs-Rest로 래핑하거나 비활성화합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
Create MODEL modelname OPTIONS(
  type = 'gradient_boosted_tree_classifier'
) AS
  select col1, col2, col3 from training-dataset
```

## [!DNL Linear Support Vector Classifier] (LinearSVC) {#linear-support-vector-classifier}

[!DNL Linear Support Vector Classifier] (LinearSVC)은 고차원 공간에서 데이터를 분류하기 위한 초평면을 구성합니다. 클래스 간의 여백을 최대화하여 분류 오류를 최소화할 수 있습니다.

**매개 변수**

아래 표에는 [!DNL Linear Support Vector Classifier (LinearSVC)]의 성능을 구성하고 최적화하는 데 필요한 주요 매개 변수가 나와 있습니다.

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|------------------------------------------------------------------------------------------------------|
| `MAX_ITER` | 알고리즘을 실행할 최대 반복 횟수입니다. | 100 | (>= 0) |
| `AGGREGATION_DEPTH` | 트리 집계의 깊이입니다. 이 매개 변수는 네트워크 통신 오버헤드를 줄이는 데 사용됩니다. | 2 | 모든 양의 정수 |
| `FIT_INTERCEPT` | 절편 용어에 맞는지 여부. | `true` | `true`, `false` |
| `TOL` | 이 매개 변수는 반복을 중지할 임계값을 결정합니다. | 1E-6 | (>= 0) |
| `MAX_BLOCK_SIZE_IN_MB` | 입력 데이터를 블록으로 스택하기 위한 최대 메모리(MB)입니다. 매개 변수를 `0`(으)로 설정하면 최적의 값이 자동으로 선택됩니다(일반적으로 약 1MB). | 0.0 | (>= 0) |
| `REG_PARAM` | 모델 복잡성을 제어하고 오버피팅을 방지하는 데 도움이 되는 정규화 매개변수입니다. | 0.0 | (>= 0) |
| `STANDARDIZATION` | 이 매개변수는 모델을 조정하기 전에 교육 피쳐를 표준화할지 여부를 나타냅니다. | `true` | `true`, `false` |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 임의의 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 임의의 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 이 알고리즘을 One-vs-Rest로 래핑하거나 비활성화합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
Create MODEL modelname OPTIONS(
  type = 'linear_svc_classifier'
) AS
  select col1, col2, col3 from training-dataset
```

## [!DNL Logistic Regression] {#logistic-regression}

[!DNL Logistic Regression]은(는) 이진 분류 작업에 사용되는 감독되는 알고리즘입니다. 로지스틱 함수를 사용하여 인스턴스가 클래스에 속할 확률을 모델링하고 더 높은 확률로 인스턴스를 클래스에 할당합니다. 이는 데이터를 두 가지 범주 중 하나로 분리하는 것이 목표인 문제에 적합하다.

**매개 변수**

아래 표에서는 [!DNL Logistic Regression]의 성능을 구성하고 최적화하기 위한 주요 매개 변수에 대해 설명합니다.

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|----------------|
| `MAX_ITER` | 알고리즘이 실행하는 최대 반복 횟수입니다. | 100 | (>= 0) |
| `REGPARAM` | 정규화 파라미터는 모델의 복잡도를 제어하는 데 사용된다. | 0.0 | (>= 0) |
| `ELASTICNETPARAM` | `ElasticNet` 혼합 매개 변수는 L1(올가미) 벌점과 L2(올가미) 벌점 사이의 균형을 제어합니다. 값이 0이면 L2 페널티(계수의 크기를 줄이는 Ridge)가 적용되고, 값이 1이면 L1 페널티(일부 계수를 0으로 설정하여 희소성을 유도하는 Lasso)가 적용됩니다. | 0.0 | (>= 0, &lt;= 1) |

{style="table-layout:auto"}

**예**

```sql
Create MODEL modelname OPTIONS(
  type = 'logistic_reg'
) AS
  select col1, col2, col3 from training-dataset
```

## [!DNL Multilayer Perceptron Classifier] {#multilayer-perceptron-classifier}

[!DNL Multilayer Perceptron Classifier] (MLPC)는 피드포워드 인공 신경망 분류자입니다. 이는 완전히 연결된 여러 노드의 레이어로 구성되며, 각 노드는 입력의 가중 선형 조합과 활성화 함수를 적용합니다. MLPC는 비선형 결정 경계가 필요한 복잡한 분류 작업에 사용됩니다.

**매개 변수**

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------------------------------|
| `MAX_ITER` | 알고리즘을 실행할 최대 반복 횟수입니다. | 100 | (>= 0) |
| `BLOCK_SIZE` | 파티션 내의 행렬에 입력 데이터를 스택하기 위한 블록 크기입니다. 블록 크기가 파티션의 나머지 데이터를 초과하면 그에 따라 조정됩니다. | 128 | (>= 0) |
| `STEP_SIZE` | 최적화의 각 반복에 사용되는 단계 크기입니다(해결자 `gd`에만 적용 가능). | 0.03 | (> 0) |
| `TOL` | 최적화를 위한 수렴 허용 오차. | `1E-6` | (>= 0) |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `SEED` | 알고리즘에서 무작위 프로세스를 제어하기 위한 무작위 시드. | 설정되지 않음 | 모든 64비트 숫자 |
| `PROBABILITY_COL` | 예측된 클래스 조건부 확률에 대한 열 이름입니다. 이러한 점수는 정확한 확률보다는 신뢰도 점수로 취급되어야 한다. | &quot;probability&quot; | 모든 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 모든 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 이 알고리즘을 One-vs-Rest로 래핑하거나 비활성화합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
CREATE MODEL modelname OPTIONS(
  type = 'multilayer_perceptron_classifier'
) AS
  select col1, col2, col3 from training-dataset
```

## [!DNL Naive Bayes Classifier] {#naive-bayes-classifier}

[!DNL Naive Bayes Classifier]은(는) 기능 간에 강력한(순진한) 독립성 가정을 가진 베이즈 정리에 기반한 간단한 확률론적 다중 클래스 분류자입니다. 훈련 데이터에 대한 단일 패스로 조건부 확률을 계산하여 각 레이블이 지정된 각 기능의 조건부 확률 분포를 계산하여 효율적으로 훈련합니다. 예측의 경우, 관측치가 주어진 각 레이블의 조건부 확률 분포를 계산하기 위해 베이즈 정리를 사용한다.

**매개 변수**

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------------------------------------|
| `MODEL_TYPE` | 모델 유형을 지정합니다. 지원되는 옵션은 `"multinomial"`, `"complement"`, `"bernoulli"` 및 `"gaussian"`입니다. 모델 유형은 대/소문자를 구분합니다. | &quot;다항식&quot; | `"multinomial"`, `"complement"`, `"bernoulli"`, `"gaussian"` |
| `SMOOTHING` | 평활 파라미터는 범주형 데이터에서 영빈도 문제를 핸들링하는 데 사용된다. | 1.0 | (>= 0) |
| `PROBABILITY_COL` | 이 매개 변수는 예측된 클래스 조건부 확률에 대한 열 이름을 지정합니다. 참고: 모든 모델이 잘 보정된 확률 추정치를 제공하는 것은 아닙니다. 이러한 값을 정확한 확률보다는 신뢰도로 취급하십시오. | &quot;probability&quot; | 모든 문자열 |
| `WEIGHT_COL` | 인스턴스 가중치의 열 이름입니다. 설정되지 않았거나 비어 있으면 모든 인스턴스 가중치는 `1.0`(으)로 처리됩니다. | 설정되지 않음 | 모든 문자열 |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 모든 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 One-vs-Rest를 사용할지 여부를 지정합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
CREATE MODEL modelname OPTIONS(
  type = 'naive_bayes_classifier'
) AS
  SELECT col1, col2, col3 FROM training-dataset
```

## [!DNL Random Forest Classifier] {#random-forest-classifier}

[!DNL Random Forest Classifier]은(는) 교육 중에 여러 결정 트리를 만드는 앙상블 학습 알고리즘입니다. 예측의 평균화와 분류 작업을 위해 트리의 대다수가 선택한 클래스를 선택함으로써 과적합을 완화한다.

**매개 변수**

| 매개변수 | 설명 | 기본 값 | 가능한 값 |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|------------------------------------------------------------------------------------------------------|
| `MAX_BINS` | 최대 빈 수는 연속 피쳐가 어떻게 이산적인 간격으로 분할되는가를 결정합니다. 이 기능은 각 의사 결정 트리 노드에서 기능이 분할되는 방식에 영향을 줍니다. 저장소가 많을수록 세부기간이 높아집니다. | 32 | 은(는) 적어도 2개이어야 하며 모든 카테고리 기능에 있는 카테고리 수보다 크거나 같아야 합니다. |
| `CACHE_NODE_IDS` | `false`이면 알고리즘이 트리를 실행자에게 전달하여 인스턴스와 노드를 일치시킵니다. `true`이면 알고리즘이 각 인스턴스에 대한 노드 ID를 캐시하여 교육 속도를 높입니다. | `false` | `true`, `false` |
| `CHECKPOINT_INTERVAL` | 캐시된 노드 ID를 체크포인팅하는 빈도를 지정합니다. 예를 들어 `10`은(는) 10번 반복할 때마다 캐시가 검사점을 지정함을 의미합니다. | 10 | (>= 1) |
| `IMPURITY` | 정보 이익 계산에 사용되는 기준(대소문자 구분 안 함). | &quot;gini&quot; | `entropy`, `gini` |
| `MAX_DEPTH` | 트리의 최대 깊이입니다(음수가 아님). 예를 들어, 깊이 `0`은(는) 1개의 리프 노드, 깊이 `1`은(는) 1개의 내부 노드 및 2개의 리프 노드를 의미합니다. | 5 | (>= 0) |
| `MIN_INFO_GAIN` | 분할이 트리 노드에서 고려되는 데 필요한 최소 정보 이득입니다. | 0.0 | (>= 0.0) |
| `MIN_WEIGHT_FRACTION_PER_NODE` | 분할 후 각 소아가 보유해야 하는 가중치가 적용된 표본 수의 최소 비율입니다. 분할로 인해 하위 항목 중 하나에 있는 총 중량의 일부가 이 값보다 작아지면 무시됩니다. | 0.0 | (>= 0.0, &lt;= 0.5) |
| `MIN_INSTANCES_PER_NODE` | 분할 후 각 하위 항목이 가져야 하는 최소 인스턴스 수입니다. 분할로 인해 이 값보다 적은 인스턴스가 생성되면 분할이 무시됩니다. | 1 | (>= 1) |
| `MAX_MEMORY_IN_MB` | 히스토그램 집계에 할당된 최대 메모리(MB)입니다. 이 값이 너무 작으면 반복당 하나의 노드만 분할되고 해당 집계는 이 크기를 초과할 수 있습니다. | 256 | (>= 1) |
| `PREDICTION_COL` | 예측 출력의 열 이름입니다. | &quot;prediction&quot; | 모든 문자열 |
| `WEIGHT_COL` | 열 이름(예: 가중치)입니다. 설정되지 않았거나 비어 있으면 모든 인스턴스 가중치는 `1.0`(으)로 처리됩니다. | 설정되지 않음 | 모든 유효한 열 이름 또는 비어 있음 |
| `SEED` | 알고리즘에서 무작위 프로세스를 제어하는 데 사용되는 무작위 시드입니다. | -1689246527 | 모든 64비트 숫자 |
| `BOOTSTRAP` | 수목을 조성할 때 부트스트랩 표본을 사용하는지 여부. | `true` | `true`, `false` |
| `NUM_TREES` | 훈련시킬 나무의 수. `1`이면 부트스트래핑이 사용되지 않습니다. `1`보다 큰 경우 부트스트래핑이 적용됩니다. | 20 | (>= 1) |
| `SUBSAMPLING_RATE` | 각 의사 결정 트리를 학습하는 데 사용되는 교육 데이터의 비율입니다. | 1.0 | (> 0, &lt;= 1) |
| `LEAF_COL` | 각 트리에서 각 인스턴스의 예측된 리프 인덱스를 사전 순서로 포함하는 리프 인덱스에 대한 열 이름입니다. | &quot;&quot; | 모든 문자열 |
| `PROBABILITY_COL` | 예측된 클래스 조건부 확률에 대한 열 이름입니다. 이러한 점수는 정확한 확률보다는 신뢰도 점수로 취급되어야 한다. | 설정되지 않음 | 모든 문자열 |
| `RAW_PREDICTION_COL` | 원시 예측 값의 열 이름(신뢰도라고도 함)입니다. | &quot;rawPrediction&quot; | 모든 문자열 |
| `ONE_VS_REST` | 다중 클래스 분류에 대해 One-vs-Rest를 사용할지 여부를 지정합니다. | `false` | `true`, `false` |

{style="table-layout:auto"}

**예**

```sql
Create MODEL modelname OPTIONS(
  type = 'random_forest_classifier'
) AS
  select col1, col2, col3 from training-dataset
```

## 다음 단계

이 문서를 읽고 나면 이제 다양한 분류 알고리즘을 구성하고 사용하는 방법을 알 수 있습니다. 다음으로 [회귀](./regression.md) 및 [클러스터링](./clustering.md)에 대한 문서를 참조하여 다른 유형의 고급 통계 모델에 대해 알아보십시오.
